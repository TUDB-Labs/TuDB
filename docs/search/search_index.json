{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TuDB \u00b6 What is TuDB? \u00b6 TuDB is a cloud native graph database. TuDB aims to support Hybrid Transactional and Analytical Processing (HTAP) workloads. It is compatible with Cypher and features horizontal scalability, strong consistency, and high availability. TuDB aims to provide a unified graph process interface for constructing and managing graphs on different key-value storage engines, such as RocksDB, HBase, and TiKV. Quick Start \u00b6 Installation \u00b6 You can install TuDB by following these steps: Download or clone the repo locally. Install Maven . Run the following to build TuDB binary: mvn clean compile install Examples \u00b6 Below is an example to start the server and client locally and interact with the database via Cypher queries. For more examples, please check out the examples folder . import org . apache . commons . io . FileUtils import org . grapheco . lynx . types . structural .{ LynxNode , LynxRelationship } import org . grapheco . tudb .{ TuDBServer , TuDBServerContext } import org . grapheco . tudb . client . TuDBClient import org . grapheco . tudb . test . TestUtils import java . io . File object CypherExample { val port = 7600 var server : TuDBServer = _ var client : TuDBClient = _ def main ( args : Array [ String ]): Unit = { val dbPath : String = s\" ${ TestUtils . getModuleRootPath } /testSpace/testBase\" server = startServer ( dbPath , port ) startClient () createNode () queryNode () createRelation () queryRelation () stopClient () shutdownServer () } def startServer ( dbPath : String , port : Int , indexUrl : String = \"tudb://index?type=dummy\" ): TuDBServer = { FileUtils . deleteDirectory ( new File ( dbPath )) val serverContext = new TuDBServerContext () serverContext . setPort ( port ) serverContext . setDataPath ( dbPath ) serverContext . setIndexUri ( indexUrl ) val server = new TuDBServer ( serverContext ) new Thread ( new Runnable { override def run (): Unit = server . start () }). start () server } def shutdownServer () = server . shutdown () def startClient (): Unit = { client = new TuDBClient ( \"127.0.0.1\" , port ) } def stopClient () = client . shutdown () def createNode (): Unit = { client . query ( \"create (n:DataBase{name:'TuDB'})\" ) client . query ( \"create (n: Company{name:'TUDB'})\" ) } def queryNode (): Unit = { val res = client . query ( \"match (n) return n\" ) println () println ( \"Query Node result: \" , res ) println () } def createRelation (): Unit = { client . query ( \"\"\" |match (n:DataBase{name:'TuDB'}) |match (c: Company{name:'TUDB'}) |create (n)-[r: belongTo{year: 2022}]->(c) |\"\"\" . stripMargin ) } def queryRelation (): Unit = { val res = client . query ( \"match (n)-[r: belongTo]->(m) return n,r,m\" ) println () println ( \"Query Relation Result: \" , res ) } } Development \u00b6 You can run all the tests via the following: mvn clean install test You can start a database instance locally via ./bin/start or start an interactive shell via ./bin/cypher . For more details on how to contribute, please check out our contributing guide .","title":"Home"},{"location":"#tudb","text":"","title":"TuDB"},{"location":"#what-is-tudb","text":"TuDB is a cloud native graph database. TuDB aims to support Hybrid Transactional and Analytical Processing (HTAP) workloads. It is compatible with Cypher and features horizontal scalability, strong consistency, and high availability. TuDB aims to provide a unified graph process interface for constructing and managing graphs on different key-value storage engines, such as RocksDB, HBase, and TiKV.","title":"What is TuDB?"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installation","text":"You can install TuDB by following these steps: Download or clone the repo locally. Install Maven . Run the following to build TuDB binary: mvn clean compile install","title":"Installation"},{"location":"#examples","text":"Below is an example to start the server and client locally and interact with the database via Cypher queries. For more examples, please check out the examples folder . import org . apache . commons . io . FileUtils import org . grapheco . lynx . types . structural .{ LynxNode , LynxRelationship } import org . grapheco . tudb .{ TuDBServer , TuDBServerContext } import org . grapheco . tudb . client . TuDBClient import org . grapheco . tudb . test . TestUtils import java . io . File object CypherExample { val port = 7600 var server : TuDBServer = _ var client : TuDBClient = _ def main ( args : Array [ String ]): Unit = { val dbPath : String = s\" ${ TestUtils . getModuleRootPath } /testSpace/testBase\" server = startServer ( dbPath , port ) startClient () createNode () queryNode () createRelation () queryRelation () stopClient () shutdownServer () } def startServer ( dbPath : String , port : Int , indexUrl : String = \"tudb://index?type=dummy\" ): TuDBServer = { FileUtils . deleteDirectory ( new File ( dbPath )) val serverContext = new TuDBServerContext () serverContext . setPort ( port ) serverContext . setDataPath ( dbPath ) serverContext . setIndexUri ( indexUrl ) val server = new TuDBServer ( serverContext ) new Thread ( new Runnable { override def run (): Unit = server . start () }). start () server } def shutdownServer () = server . shutdown () def startClient (): Unit = { client = new TuDBClient ( \"127.0.0.1\" , port ) } def stopClient () = client . shutdown () def createNode (): Unit = { client . query ( \"create (n:DataBase{name:'TuDB'})\" ) client . query ( \"create (n: Company{name:'TUDB'})\" ) } def queryNode (): Unit = { val res = client . query ( \"match (n) return n\" ) println () println ( \"Query Node result: \" , res ) println () } def createRelation (): Unit = { client . query ( \"\"\" |match (n:DataBase{name:'TuDB'}) |match (c: Company{name:'TUDB'}) |create (n)-[r: belongTo{year: 2022}]->(c) |\"\"\" . stripMargin ) } def queryRelation (): Unit = { val res = client . query ( \"match (n)-[r: belongTo]->(m) return n,r,m\" ) println () println ( \"Query Relation Result: \" , res ) } }","title":"Examples"},{"location":"#development","text":"You can run all the tests via the following: mvn clean install test You can start a database instance locally via ./bin/start or start an interactive shell via ./bin/cypher . For more details on how to contribute, please check out our contributing guide .","title":"Development"},{"location":"CONTRIBUTING/","text":"Contributing Guide \u00b6 Welcome to TuDB's contributing guide! Local Setup \u00b6 Prerequisites \u00b6 Please install the following before walking through the rest of this guide: Java 8 Maven Maven Configuration \u00b6 If you are a developer in China, please modify your Maven configurations in ~/.m2/settings.xml to add the Aliyun Maven Central mirror: <settings> <mirrors> <mirror> <id> alimaven </id> <mirrorOf> central </mirrorOf> <name> aliyun maven </name> <url> https://maven.aliyun.com/repository/public/ </url> </mirror> </mirrors> </settings> Testing \u00b6 Run the following command to run the test suite: mvn -B clean install test --file pom.xml . Pre-commit Checks \u00b6 We run several checks before every commit automatically with pre-commit . Install pre-commit to run the required checks when you commit your changes. Once it's installed, run pre-commit install to install the hooks that will be run automatically when you git commit your changes. You can also run it via pre-commit run on your changes or pre-commit run --all to run the checks on all files. If you'd like to uninstall the pre-commit hooks, run pre-commit uninstall . Submit Changes \u00b6 To submit a change, please follow the following steps: 1. Create a fork and push changes to a branch in your fork. 1. Create a pull request from the branch in your fork . Code Style \u00b6 For Python code, We follows PEP 8 with one exception: lines can be up to 100 characters in length, not 79. For Java code, We follows Oracle\u2019s Java code conventions and Scala guidelines below. The latter is preferred. For Scala code, We follows the official Scala style guide and Databricks Scala guide . The latter is preferred. To format Scala code, run ./dev/scalafmt prior to submitting a PR.","title":"Contributing Guide"},{"location":"CONTRIBUTING/#contributing-guide","text":"Welcome to TuDB's contributing guide!","title":"Contributing Guide"},{"location":"CONTRIBUTING/#local-setup","text":"","title":"Local Setup"},{"location":"CONTRIBUTING/#prerequisites","text":"Please install the following before walking through the rest of this guide: Java 8 Maven","title":"Prerequisites"},{"location":"CONTRIBUTING/#maven-configuration","text":"If you are a developer in China, please modify your Maven configurations in ~/.m2/settings.xml to add the Aliyun Maven Central mirror: <settings> <mirrors> <mirror> <id> alimaven </id> <mirrorOf> central </mirrorOf> <name> aliyun maven </name> <url> https://maven.aliyun.com/repository/public/ </url> </mirror> </mirrors> </settings>","title":"Maven Configuration"},{"location":"CONTRIBUTING/#testing","text":"Run the following command to run the test suite: mvn -B clean install test --file pom.xml .","title":"Testing"},{"location":"CONTRIBUTING/#pre-commit-checks","text":"We run several checks before every commit automatically with pre-commit . Install pre-commit to run the required checks when you commit your changes. Once it's installed, run pre-commit install to install the hooks that will be run automatically when you git commit your changes. You can also run it via pre-commit run on your changes or pre-commit run --all to run the checks on all files. If you'd like to uninstall the pre-commit hooks, run pre-commit uninstall .","title":"Pre-commit Checks"},{"location":"CONTRIBUTING/#submit-changes","text":"To submit a change, please follow the following steps: 1. Create a fork and push changes to a branch in your fork. 1. Create a pull request from the branch in your fork .","title":"Submit Changes"},{"location":"CONTRIBUTING/#code-style","text":"For Python code, We follows PEP 8 with one exception: lines can be up to 100 characters in length, not 79. For Java code, We follows Oracle\u2019s Java code conventions and Scala guidelines below. The latter is preferred. For Scala code, We follows the official Scala style guide and Databricks Scala guide . The latter is preferred. To format Scala code, run ./dev/scalafmt prior to submitting a PR.","title":"Code Style"},{"location":"performance-report/","text":"Performance Report \u00b6 Experiment Setup \u00b6 Environment \u00b6 OS: CentOS7 CPU: 26 cores DISK: SSD Mem: 376G JDK-version: 1.8 Scala-version: 2.12.8 Dataset \u00b6 LDBC-SNB Name # Nodes # Relationships SF1 3115527 17236389 SF3 8879918 50728269 SF10 28125740 166346601 Method \u00b6 Node test: choose the first node from each label, then test different API avg time cost. relation test: choose the first end node from each relation type, then test different API avg time cost. Notice: 1. allNodes : test the time cost of scan all node data in db. 2. findInRelations : test the time cost of return the first 10 relations of this node. Performance Report \u00b6 SF1 Performance \u00b6 NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 2444 ms findInRelations 486299 us getNodeById 187896 us getRelationById 11987 us nodeSetProperty 961045 us relationSetProperty 202494 us nodeRemoveProperty 773296 us relationRemoveProperty 158453 us deleteNode 226737 us deleteRelation 36684 us addNode 35091 us addRelation 32427 us SF3 Performance \u00b6 NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 7145 ms findInRelations 472155 us getNodeById 368505 us getRelationById 15265 us nodeSetProperty 941363 us relationSetProperty 198322 us nodeRemoveProperty 644449 us relationRemoveProperty 171554 us deleteNode 169693 us deleteRelation 33459 us addNode 32525 us addRelation 32301 us SF10 Performance \u00b6 NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 23491 ms findInRelations 589703 us getNodeById 272012 us getRelationById 18101 us nodeSetProperty 941371 us relationSetProperty 193019 us nodeRemoveProperty 865621 us relationRemoveProperty 179682 us deleteNode 197920 us deleteRelation 37630 us addNode 34704 us addRelation 36308 us","title":"Performance Report"},{"location":"performance-report/#performance-report","text":"","title":"Performance Report"},{"location":"performance-report/#experiment-setup","text":"","title":"Experiment Setup"},{"location":"performance-report/#environment","text":"OS: CentOS7 CPU: 26 cores DISK: SSD Mem: 376G JDK-version: 1.8 Scala-version: 2.12.8","title":"Environment"},{"location":"performance-report/#dataset","text":"LDBC-SNB Name # Nodes # Relationships SF1 3115527 17236389 SF3 8879918 50728269 SF10 28125740 166346601","title":"Dataset"},{"location":"performance-report/#method","text":"Node test: choose the first node from each label, then test different API avg time cost. relation test: choose the first end node from each relation type, then test different API avg time cost. Notice: 1. allNodes : test the time cost of scan all node data in db. 2. findInRelations : test the time cost of return the first 10 relations of this node.","title":"Method"},{"location":"performance-report/#performance-report_1","text":"","title":"Performance Report"},{"location":"performance-report/#sf1-performance","text":"NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 2444 ms findInRelations 486299 us getNodeById 187896 us getRelationById 11987 us nodeSetProperty 961045 us relationSetProperty 202494 us nodeRemoveProperty 773296 us relationRemoveProperty 158453 us deleteNode 226737 us deleteRelation 36684 us addNode 35091 us addRelation 32427 us","title":"SF1 Performance"},{"location":"performance-report/#sf3-performance","text":"NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 7145 ms findInRelations 472155 us getNodeById 368505 us getRelationById 15265 us nodeSetProperty 941363 us relationSetProperty 198322 us nodeRemoveProperty 644449 us relationRemoveProperty 171554 us deleteNode 169693 us deleteRelation 33459 us addNode 32525 us addRelation 32301 us","title":"SF3 Performance"},{"location":"performance-report/#sf10-performance","text":"NodeStoreAPI avg time cost RelationStoreApi avg time cost allNodes 23491 ms findInRelations 589703 us getNodeById 272012 us getRelationById 18101 us nodeSetProperty 941371 us relationSetProperty 193019 us nodeRemoveProperty 865621 us relationRemoveProperty 179682 us deleteNode 197920 us deleteRelation 37630 us addNode 34704 us addRelation 36308 us","title":"SF10 Performance"},{"location":"roadmap/","text":"Roadmap \u00b6 TBA","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"TBA","title":"Roadmap"},{"location":"designs/cn/metrics/","text":"Metrics Expose Design \u8bbe\u8ba1\u76ee\u6807 \u7cfb\u7edfmetrics\u7528\u4e8e\u76d1\u63a7\u56fe\u6570\u636e\u5e93\u7cfb\u7edf\u8fd0\u884c\u7684\u60c5\u51b5\u3002\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u65b9\u9762\uff1a \u7279\u5b9a\u64cd\u4f5c\u9700\u8981\u7684\u65f6\u95f4\u3002\u4f8b\u5982\uff0c\u5355\u4e2a\u67e5\u8be2\u7684\u8017\u65f6\u3002 \u81ea\u5b9a\u4e49\u5173\u952e\u53c2\u6570\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4e0d\u540c\u7684operator\uff0c\u53ef\u80fd\u9700\u8981\u8f93\u51fa\u4e0d\u540c\u7684\u4fe1\u606f\u3002\u4f8b\u5982\u8bfb\u5199\u64cd\u4f5c\u65f6\uff0c\u8bfb\u5199\u7684\u6570\u636e\u5927\u5c0f\u3002 \u7cfb\u7edf\u8d44\u6e90\u53c2\u6570\u3002\u4f8b\u5982\uff0cCPU\u548c\u5185\u5b58\u5229\u7528\u7387\u3002 \u6211\u4eec\u5c06metrics expose\u5206\u6210\u4e24\u4e2a\u9636\u6bb5\u5b8c\u6210\u3002 1.1 \u7b2c\u4e00\u9636\u6bb5 \u7b2c\u4e00\u9636\u6bb5\u4e3b\u8981\u6536\u96c6\u5355\u4e2aquery\u7684\u6027\u80fd\u4fe1\u606f\uff1a \u5355\u4e2aquery\u7684\u8017\u65f6 \u8fd9\u4e2aquery\u91cc\u9762\uff0c\u6bcf\u4e2aoperator\u7684\u8017\u65f6 \u6bcf\u4e2aoperator\u76f8\u5173\u7684\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u6bcf\u4e2aoperator\u662f\u4e0d\u540c\u7684 1.2 \u7b2c\u4e8c\u9636\u6bb5 \u8fd9\u4e00\u9636\u6bb5\u4e3b\u8981\u5b8c\u6210\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u7684\u76d1\u63a7\uff1a \u6240\u6709query\u6027\u80fd\u76f8\u5173\u53c2\u6570\u7684\u805a\u5408\u3002\u4f8b\u598295% query\u7684\u8017\u65f6\u3002 \u7cfb\u7edf\u7ea7\u522b\u7684\u6027\u80fd\u548c\u8d44\u6e90\u53c2\u6570\u3002\u4f8b\u5982\u541e\u5410\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002 \u73b0\u9636\u6bb5\u6211\u4eec\u4e3b\u8981\u4e13\u6ce8\u4e8e\u7b2c\u4e00\u9636\u6bb5\u7684\u5b9e\u73b0\u3002 Metrics\u8bbe\u8ba1 \u8fd9\u91cc\u6211\u4eec\u63d0\u51fa\u4e00\u4e2ametrics domain\u7684\u8bbe\u8ba1\u3002\u6bcf\u4e2adomian\u91cc\u9762\u53ea\u5305\u542b\u8fd9\u4e2adomain\u76f8\u5173\u7684\u65e5\u5fd7\u3002\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e00\u4e2adomian\u7684\u65e5\u5fd7\u8fdb\u884c\u805a\u5408\uff0c\u5206\u89e3\u5e76\u901a\u8fc7\u4e0d\u540c\u7684view\u5c55\u793a\u3002\u4f46\u4e0d\u540cdomian\u7684\u65e5\u5fd7\u4e0d\u80fd\u8fdb\u884c\u805a\u5408\u5c55\u793a\u3002 \u65e5\u5fd7\u4e8b\u4f8b\uff1a DomainID: \u65e5\u5fd7\u6240\u5c5e\u7684domian\u7684id\u3002\u8f93\u51fa\u4efb\u4f55\u65e5\u5fd7\u524d\uff0c\u5fc5\u987b\u5148\u521b\u5efa\u4e00\u4e2a\u65e5\u5fd7domian\u3002Domain id\u53ef\u4ee5\u81ea\u5b9a\u4e49\u3002\u5982\u679c\u7528\u6237\u6ca1\u6709\u5b9a\u4e49\uff0c\u7cfb\u7edf\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u72ec\u7279\u7684id\u3002 Labels\uff1a\u5b8c\u5168\u7531\u7528\u6237\u81ea\u5b9a\u4e49\uff0c\u4e00\u6761\u65e5\u5fd7\u53ef\u6709\u6709\u591a\u4e2alabels\uff08\u7528;\u9694\u5f00\uff09\uff0c\u4e5f\u53ef\u4ee5\u4e3a\u7a7a\u3002 Timestamp\uff1a\u8bb0\u5f55\u65e5\u5fd7\u7684\u65f6\u95f4\u3002 Type\uff1a\u65e5\u5fd7\u7684\u6570\u636e\u683c\u5f0f\u3002\u4f8b\u5982\uff0cstring, int, float\u7b49\u3002 Log: \u65e5\u5fd7\u3002 \u4e0b\u9762\u6211\u4eec\u7528\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u89e3\u91cametrics\u7684\u4f7f\u7528\u3002 \u5047\u8bbe\u6211\u4eec\u9700\u8981\u8ffd\u8e2a\u4e00\u4e2aquery\u7684\u6027\u80fd\u3002Domain id\u5c31\u662f\u4e00\u4e2aquery\u7684id\u3002 \u5982\u679c\u6211\u4eec\u9700\u8981\u8bb0\u5f55\u8fd9\u4e2aquery\u7684latency\uff1a \u5982\u679c\u6211\u4eec\u9700\u8981\u8bb0\u5f55\u67d0\u4e2aoperator\u7684latency\uff1a \u540c\u7406\uff0c\u5982\u679c\u6211\u4eec\u9700\u8981\u8bb0\u5f55\u67d0\u4e2aoperator\u7684\u5176\u4ed6\u53c2\u6570\uff1a \u5728\u8fdb\u884c\u5c55\u793a\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u6709\u76f8\u540clabel\u7684\u65e5\u5fd7\u6570\u636e\u8fdb\u884c\u805a\u5408\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u5e0c\u671b\u4e86\u89e3\u6bcf\u4e2aoperator latency\u7684\u5360\u6bd4\uff0c\u5c31\u53ef\u4ee5\u5bf9Query1\u7684\u65e5\u5fd7\u4e2d\uff0c\u8fc7\u6ee4\u51fa\u6709OperatorLog\u548cLatency\u4e24\u4e2alabel\u7684\u65e5\u5fd7\u8fdb\u884c\u805a\u5408\u3002\u5982\u679c\u6211\u4eec\u5e0c\u671b\u67e5\u770boperator1\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u53ea\u9700\u8981\u8fc7\u6ee4\u51fa\u6709OperatorLog\u548cOperator1\u4e24\u4e2alabel\u7684\u65e5\u5fd7\u3002","title":"Metrics"},{"location":"designs/cn/performance-based-refactoring/","text":"\u9488\u5bf9\u6027\u80fd\u8c03\u4f18\u7684\u67e5\u8be2\u5f15\u64ce\u6267\u884c\u5c42\u91cd\u6784 \u00b6 \u95ee\u9898 \u00b6 \u76ee\u524dTuDB\u7684\u67e5\u8be2\u5f15\u64ceLynx\u867d\u7136\u5b9e\u73b0\u4e86\u57fa\u672c\u529f\u80fd\uff0c\u4f46\u662f\u4ee3\u7801\u7ed3\u6784\u4e0d\u591f\u6e05\u6670\uff0c\u5b9e\u73b0\u4e0d\u591f\u9ad8\u6548\uff0c\u6267\u884c\u5c42\u95ee\u9898\u5c24\u4e3a\u660e\u663e\uff0c\u751a\u81f3\u4f1a\u963b\u788d\u6027\u80fd\u5206\u6790\u3002\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u5bf9\u5176\u91cd\u6784\uff0c\u8ba9\u6027\u80fd\u5206\u6790\u53d8\u5f97\u53ef\u80fd\uff0c\u4f18\u5316\u4e00\u4e9b\u663e\u800c\u6613\u89c1\u7684\u4f4e\u6548\u5b9e\u73b0\uff0c\u540c\u65f6\u63d0\u9ad8\u5176\u4ee3\u7801\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u5bb9\u6613\u8ba9\u4eba\u7406\u89e3\u548c\u4fee\u6539\u3002 \u5206\u6790 \u00b6 \u76ee\u524dLynx\u628a\u4e00\u4e2a\u67e5\u8be2\u8f6c\u6362\u4e3a\u7269\u7406\u8ba1\u5212\u540e\uff0c\u8868\u9762\u4e0a\u6267\u884c\u53d1\u751f\u5728\u5404\u4e2a\u7269\u7406\u8282\u70b9\u5bf9PPTNode::execute()\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u5176\u7ed3\u679c\u4e3a\u4e00\u4e2aDataFrame\u5bf9\u8c61\uff1b\u5b9e\u9645\u7684\u6267\u884c\u65b9\u5f0f\u8bb0\u5f55\u5728DataFrame\u5bf9\u8c61\u7684records Iterator\u91cc\uff0c\u4ee5\u533f\u540d\u51fd\u6570\u7684\u5f62\u5f0f\u5b58\u5728\uff08\u53c2\u89c1DefaultDataFrameOperator\uff09\uff0c\u5e76\u5728\u7528\u6237\u771f\u6b63\u4f7f\u7528\u8fd9\u4e2aIterator\u63d0\u53d6\u7ed3\u679c\u65f6\u89e6\u53d1\u6267\u884c\uff08\u76ee\u524d\u5728QueryService\u751f\u6210json\u7ed3\u679c\u65f6\u8c03\u7528\uff09\u3002\u8868\u9762\u4e0a\u6211\u4eec\u5728\u201c\u6267\u884c\u201d\u7269\u7406\u8ba1\u5212\uff0c\u5b9e\u9645\u4e0a\u662f\u751f\u6210\u4e86\u4e00\u4e2a\u6267\u884c\u8ba1\u5212\u6811\u3002\u6811\u7684\u8282\u70b9\u662f\u5404\u4e2a\u8fd0\u7b97\u7b26\u5982filter/project/join\uff0c\u6267\u884c\u65f6\u8282\u70b9\u4e4b\u95f4\u4f20\u9012\u7684\u662f\u5e26\u6709\u7279\u5b9a\u683c\u5f0f\u7684\u5355\u884c\u6570\u636e\uff0c\u5177\u4f53\u8868\u793a\u4e3aSeq[LynxValue]\u3002 \u8fd9\u6837\u6709\u5982\u4e0b\u51e0\u4e2a\u95ee\u9898\uff1a * \u8fd0\u7b97\u7b26\u6309\u884c\u4f20\u9012\u6570\u636e\uff0c\u8fd1\u4f3c\u4e8e\u89e3\u91ca\u6267\u884c\uff0cCPU\u5f00\u9500\u5f88\u9ad8\uff08\u5e76\u975e\u5f53\u524d\u74f6\u9888\uff0c\u89e3\u51b3\u65b9\u6848\u4e0d\u5728\u6b64\u8bbe\u8ba1\u8303\u56f4\u5185\uff09\uff1b\u800c\u4e14\u5bfc\u81f4\u65e0\u6cd5\u8bb0\u5f55\u6bcf\u4e2a\u7b97\u5b50\u7528\u4e86\u591a\u957f\u65f6\u95f4\uff0c\u6bcf\u884c\u8bb0\u5f55\u4e00\u6b21\u4e0d\u51c6\u786e\u800c\u4e14\u5f00\u9500\u5927\u3002 * \u67e5\u8be2\u6267\u884c\u9636\u6bb5\u7684\u6027\u80fd\u5206\u6790\u591a\u4f9d\u8d56\u4e8eProfiling\uff1aprofiling\u5de5\u5177\u6536\u96c6\u5404\u4e2a\u65f6\u95f4\u70b9\u7684stacktrace\uff0c\u800c\u540e\u901a\u8fc7stack frame\u4e2d\u51fa\u73b0\u7684\u7c7b/\u51fd\u6570\u540d\u6765\u4f30\u7b97\u5b83\u4eec\u5360\u7528\u7684\u65f6\u95f4\uff0c\u7ed3\u679c\u591a\u4e3a\u706b\u7130\u56fe\u3002\u8fd0\u7b97\u7b26\u5747\u4e3a\u533f\u540d\u51fd\u6570\u65f6\uff0cprofiling\u65e0\u6cd5\u6709\u6548\u6307\u51fa\u6bcf\u4e2a\u8fd0\u7b97\u7b26\u4f7f\u7528\u7684\u65f6\u95f4\u3002 * \u8fd0\u7b97\u7b26\u6ca1\u6709\u4efb\u4f55\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u4e5f\u7531\u4e8e\u5168\u90fd\u662fDefaultDataFrameOperator\u91cc\u7684\u533f\u540d\u51fd\u6570\uff0c\u4e5f\u4e0d\u597d\u5355\u5143\u6d4b\u8bd5\u3002 * \u6240\u6709\u7684\u8fd0\u7b97\u7b26\u5747\u653e\u5728DefaultDataFrameOperator\u91cc\uff0c\u5bfc\u81f4\u5176\u8fc7\u4e8e\u81c3\u80bf\u3002\u8fd0\u7b97\u7b26\u8c03\u7528\u5e76\u975e\u663e\u5f0f\u51fd\u6570\u8c03\u7528\uff0c\u800c\u662f\u4f7f\u7528\u9690\u5f0f\u7c7b\u578b\u8f6c\u6362(Scala implicit def)\u76f4\u63a5\u5728DataFrame\u5bf9\u8c61\u4e0a\u8c03\u7528\uff0c\u4e0d\u591f\u76f4\u89c2\u4e5f\u4e0d\u6613\u5bfb\u627e\u3002 \u65b9\u6848 \u00b6 \u6211\u4eec\u53ef\u4ee5\u628a\u6267\u884c\u8ba1\u5212\u6811\u505a\u6210\u4e00\u4e2a\u663e\u5f0f\u7684\u7ed3\u6784\u3002\u5404\u4e2a\u8fd0\u7b97\u7b26\u5212\u5206\u6210\u5355\u72ec\u7684\u7c7b\uff08\u540c\u65f6\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5\uff09\uff0c\u5e76\u7edf\u4e00\u7ee7\u627f\u4e00\u4e2a\u7c7b\u4f3cIterator\u7684\u7236\u7c7b\uff1a trait ExecutionOperator extends TreeNode { // Input operators val children: Seq[ExecutionOperator] // Prepare for processing. def Open() = { ...common logic... OpenImpl() } // To be implemented by concrete operators. def OpenSelf(); // Fills output_batch. Empty RowBatch means the end of output. def GetNext(output_batch: RowBatch) = { ...common logic... // Example way of collecting per-operator metrics. ScopedTimer timer; return GetNextImpl(output_batch); } // Interface for child class to override def GetNextImpl(output_batch: RowBatch); // Ends processing. def Close() = { ...common logic... CloseImpl() } // To be implemented by concrete operators. def CloseImpl(); // Schema of output rows. def OutputSchema() } // Example concrete operator. class FilterOperator extends ExecutionOperator { override def OpenImpl() = { children[0].Open() } override def GetNextImpl(output_batch: RowBatch): Unit = { while (!output_batch.full()) { RowBatch input_batch; children[0].GetNext(input_batch) if (input_batch.empty()) break; RunFilter(input_batch, output_batch) } } override def CloseImpl() = { children[0].Close() } } \u8fd0\u7b97\u7b26\u7684\u6574\u4e2a\u5468\u671f\u662fOpen->GetNext->Close\uff0c\u5176\u4e2dGetNext\u53ef\u4ee5\u53cd\u590d\u8c03\u7528\u76f4\u5230\u4e0d\u80fd\u518d\u8fd4\u56de\u66f4\u591a\u7ed3\u679c\u3002\u7236\u7c7bExecutionOperator\u4e2d\u5bf9\u8fd9\u4e09\u4e2a\u63a5\u53e3\u6709\u7edf\u4e00\u7684\u57fa\u672c\u5b9e\u73b0\uff0c\u7528\u6765\u505a\u4e00\u4e9b\u6240\u6709\u64cd\u4f5c\u7b26\u90fd\u901a\u7528\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982\u6536\u96c6metrics\uff0c\u5404\u64cd\u4f5c\u7b26\u72ec\u81ea\u7684\u903b\u8f91\u5728\u5bf9\u4e8e\u7684Impl\u65b9\u6cd5\u91cc\u5b9e\u73b0\u3002\u5728\u6267\u884c\u8ba1\u5212\u751f\u6210\u540e\uff0c\u53ea\u9700\u50cf\u5bf9Iterator\u4e00\u6837\u4ece\u6839\u8282\u70b9\u83b7\u53d6\u8f93\u51fa\uff0c\u5404\u8282\u70b9\u5185\u90e8\u4f1a\u5c42\u7ea7\u8c03\u7528\u5404\u81ea\u5b50\u7ed3\u70b9\u7684Open/GetNext/Close\u3002 ExecutionOperator execution_plan = GenerateExecutionPlan ( physical_plan ) execution_plan . Open () while ( true ) { RowBatch output_batch ; execution_plan . GetNext ( output_batch ) ; if ( output_batch . empty ()) break ; // consume output here } execution_plan . Close () \u8fd0\u7b97\u7b26\u4e4b\u95f4\u4f20\u9012\u7684\u5355\u5143\u4e3aRowBatch\uff0c\u5305\u542b\u591a\u4e2a\u8f93\u51fa\u884c\u3002\u5177\u4f53\u884c\u6570\u53ef\u4ee5\u5148\u7528\u56fa\u5b9a\u503c\u6bd4\u5982256\u3002\u5c06\u6765\u6709\u5185\u5b58\u7ba1\u7406\u540e\u9700\u8981\u6309\u7167\u5b9e\u9645\u5360\u7528\u5185\u5b58\u5927\u5c0f\u51b3\u5b9a\u884c\u6570\u3002 class RowBatch { val rows: Seq[Seq[LynxValue]] } \u4ece\u7269\u7406\u8ba1\u5212\u5230\u6267\u884c\u8ba1\u5212\u7684\u8f6c\u6362\u65b9\u5f0f\u53ef\u4ee5\u53c2\u7167\u903b\u8f91\u8ba1\u5212\u5230\u7269\u7406\u8ba1\u5212\u7684\u8f6c\u6362\u3002 \u5b9e\u65bd \u00b6 \u6dfb\u52a0ExecutionOperator\u548cRowBatch\u57fa\u672c\u7c7b\u522b \u9488\u5bf9\u6bcf\u79cd\u8fd0\u7b97\uff0c\u6dfb\u52a0\u4e00\u4e2a\u7ee7\u627fExecutionOperator\u8fd0\u7b97\u7b26\uff0c\u903b\u8f91\u53ef\u4ee5\u7167\u6284\u76f8\u5e94\u7684\u7269\u7406\u8ba1\u5212\u8282\u70b9\u7684execute\u65b9\u6cd5\u53caDataFrameOperator\u4e2d\u7684\u8fd0\u7b97\u65b9\u6cd5\uff08\u5982select/join)\uff0c\u7136\u540e\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5\u3002\u5355\u5143\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u5355\u4e2a\u8fd0\u7b97\u7b26\u6267\u884c\u5b8c\u6574\u7684Open/GetNext/Close\u5468\u671f\uff0c\u7136\u540e\u68c0\u67e5\u7ed3\u679c\u662f\u5426\u6b63\u786e\u3002\u4e3a\u4e86\u65b9\u4fbf\u6d4b\u8bd5\uff0c\u589e\u52a0\u4e00\u4e2a\u53ef\u5b9a\u5236\u8f93\u51fa\u6570\u636eFakeOperator\uff0c\u7528\u505a\u88ab\u6d4b\u8fd0\u7b97\u7b26\u7684\u8f93\u5165\u8fd0\u7b97\u7b26\u3002 \u6240\u6709\u8fd0\u7b97\u7b26\u6dfb\u52a0\u5b8c\u6bd5\u540e\uff0c\u6dfb\u52a0ExecutionPlanGenerator\uff0c\u7528\u4e8e\u5c06\u7269\u7406\u8ba1\u5212\u8f6c\u4e3a\u6267\u884c\u8ba1\u5212\u3002 \u5c06ExecutionPlanGenerator\u5e94\u7528\u5728CypherRunner\u4e2d\uff0c\u53d6\u4ee3physicalPlan.execute()\u3002\u5904\u7406\u8f93\u51fa\u65f6\uff0c\u53ef\u4ee5\u5199\u4e00\u4e2aIterator\u5c06\u6700\u7ec8\u7684RowBatch stream\u8f6c\u4e3aLynxResult.records\u8981\u6c42\u7684stream\uff0c\u8fd9\u6837\u53ef\u907f\u514d\u4f7f\u7528LynxResult\u7684\u5176\u4ed6\u90e8\u4ef6\u3002 \u6e05\u7406DataFrame/DataFrameOperator/DataFrameOps\u53ca\u6240\u6709\u7684PPTNode\u5b50\u7c7b\u7684execute\u65b9\u6cd5","title":"\u9488\u5bf9\u6027\u80fd\u8c03\u4f18\u7684\u67e5\u8be2\u5f15\u64ce\u6267\u884c\u5c42\u91cd\u6784"},{"location":"designs/cn/performance-based-refactoring/#_1","text":"","title":"\u9488\u5bf9\u6027\u80fd\u8c03\u4f18\u7684\u67e5\u8be2\u5f15\u64ce\u6267\u884c\u5c42\u91cd\u6784"},{"location":"designs/cn/performance-based-refactoring/#_2","text":"\u76ee\u524dTuDB\u7684\u67e5\u8be2\u5f15\u64ceLynx\u867d\u7136\u5b9e\u73b0\u4e86\u57fa\u672c\u529f\u80fd\uff0c\u4f46\u662f\u4ee3\u7801\u7ed3\u6784\u4e0d\u591f\u6e05\u6670\uff0c\u5b9e\u73b0\u4e0d\u591f\u9ad8\u6548\uff0c\u6267\u884c\u5c42\u95ee\u9898\u5c24\u4e3a\u660e\u663e\uff0c\u751a\u81f3\u4f1a\u963b\u788d\u6027\u80fd\u5206\u6790\u3002\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u5bf9\u5176\u91cd\u6784\uff0c\u8ba9\u6027\u80fd\u5206\u6790\u53d8\u5f97\u53ef\u80fd\uff0c\u4f18\u5316\u4e00\u4e9b\u663e\u800c\u6613\u89c1\u7684\u4f4e\u6548\u5b9e\u73b0\uff0c\u540c\u65f6\u63d0\u9ad8\u5176\u4ee3\u7801\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u5bb9\u6613\u8ba9\u4eba\u7406\u89e3\u548c\u4fee\u6539\u3002","title":"\u95ee\u9898"},{"location":"designs/cn/performance-based-refactoring/#_3","text":"\u76ee\u524dLynx\u628a\u4e00\u4e2a\u67e5\u8be2\u8f6c\u6362\u4e3a\u7269\u7406\u8ba1\u5212\u540e\uff0c\u8868\u9762\u4e0a\u6267\u884c\u53d1\u751f\u5728\u5404\u4e2a\u7269\u7406\u8282\u70b9\u5bf9PPTNode::execute()\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u5176\u7ed3\u679c\u4e3a\u4e00\u4e2aDataFrame\u5bf9\u8c61\uff1b\u5b9e\u9645\u7684\u6267\u884c\u65b9\u5f0f\u8bb0\u5f55\u5728DataFrame\u5bf9\u8c61\u7684records Iterator\u91cc\uff0c\u4ee5\u533f\u540d\u51fd\u6570\u7684\u5f62\u5f0f\u5b58\u5728\uff08\u53c2\u89c1DefaultDataFrameOperator\uff09\uff0c\u5e76\u5728\u7528\u6237\u771f\u6b63\u4f7f\u7528\u8fd9\u4e2aIterator\u63d0\u53d6\u7ed3\u679c\u65f6\u89e6\u53d1\u6267\u884c\uff08\u76ee\u524d\u5728QueryService\u751f\u6210json\u7ed3\u679c\u65f6\u8c03\u7528\uff09\u3002\u8868\u9762\u4e0a\u6211\u4eec\u5728\u201c\u6267\u884c\u201d\u7269\u7406\u8ba1\u5212\uff0c\u5b9e\u9645\u4e0a\u662f\u751f\u6210\u4e86\u4e00\u4e2a\u6267\u884c\u8ba1\u5212\u6811\u3002\u6811\u7684\u8282\u70b9\u662f\u5404\u4e2a\u8fd0\u7b97\u7b26\u5982filter/project/join\uff0c\u6267\u884c\u65f6\u8282\u70b9\u4e4b\u95f4\u4f20\u9012\u7684\u662f\u5e26\u6709\u7279\u5b9a\u683c\u5f0f\u7684\u5355\u884c\u6570\u636e\uff0c\u5177\u4f53\u8868\u793a\u4e3aSeq[LynxValue]\u3002 \u8fd9\u6837\u6709\u5982\u4e0b\u51e0\u4e2a\u95ee\u9898\uff1a * \u8fd0\u7b97\u7b26\u6309\u884c\u4f20\u9012\u6570\u636e\uff0c\u8fd1\u4f3c\u4e8e\u89e3\u91ca\u6267\u884c\uff0cCPU\u5f00\u9500\u5f88\u9ad8\uff08\u5e76\u975e\u5f53\u524d\u74f6\u9888\uff0c\u89e3\u51b3\u65b9\u6848\u4e0d\u5728\u6b64\u8bbe\u8ba1\u8303\u56f4\u5185\uff09\uff1b\u800c\u4e14\u5bfc\u81f4\u65e0\u6cd5\u8bb0\u5f55\u6bcf\u4e2a\u7b97\u5b50\u7528\u4e86\u591a\u957f\u65f6\u95f4\uff0c\u6bcf\u884c\u8bb0\u5f55\u4e00\u6b21\u4e0d\u51c6\u786e\u800c\u4e14\u5f00\u9500\u5927\u3002 * \u67e5\u8be2\u6267\u884c\u9636\u6bb5\u7684\u6027\u80fd\u5206\u6790\u591a\u4f9d\u8d56\u4e8eProfiling\uff1aprofiling\u5de5\u5177\u6536\u96c6\u5404\u4e2a\u65f6\u95f4\u70b9\u7684stacktrace\uff0c\u800c\u540e\u901a\u8fc7stack frame\u4e2d\u51fa\u73b0\u7684\u7c7b/\u51fd\u6570\u540d\u6765\u4f30\u7b97\u5b83\u4eec\u5360\u7528\u7684\u65f6\u95f4\uff0c\u7ed3\u679c\u591a\u4e3a\u706b\u7130\u56fe\u3002\u8fd0\u7b97\u7b26\u5747\u4e3a\u533f\u540d\u51fd\u6570\u65f6\uff0cprofiling\u65e0\u6cd5\u6709\u6548\u6307\u51fa\u6bcf\u4e2a\u8fd0\u7b97\u7b26\u4f7f\u7528\u7684\u65f6\u95f4\u3002 * \u8fd0\u7b97\u7b26\u6ca1\u6709\u4efb\u4f55\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u4e5f\u7531\u4e8e\u5168\u90fd\u662fDefaultDataFrameOperator\u91cc\u7684\u533f\u540d\u51fd\u6570\uff0c\u4e5f\u4e0d\u597d\u5355\u5143\u6d4b\u8bd5\u3002 * \u6240\u6709\u7684\u8fd0\u7b97\u7b26\u5747\u653e\u5728DefaultDataFrameOperator\u91cc\uff0c\u5bfc\u81f4\u5176\u8fc7\u4e8e\u81c3\u80bf\u3002\u8fd0\u7b97\u7b26\u8c03\u7528\u5e76\u975e\u663e\u5f0f\u51fd\u6570\u8c03\u7528\uff0c\u800c\u662f\u4f7f\u7528\u9690\u5f0f\u7c7b\u578b\u8f6c\u6362(Scala implicit def)\u76f4\u63a5\u5728DataFrame\u5bf9\u8c61\u4e0a\u8c03\u7528\uff0c\u4e0d\u591f\u76f4\u89c2\u4e5f\u4e0d\u6613\u5bfb\u627e\u3002","title":"\u5206\u6790"},{"location":"designs/cn/performance-based-refactoring/#_4","text":"\u6211\u4eec\u53ef\u4ee5\u628a\u6267\u884c\u8ba1\u5212\u6811\u505a\u6210\u4e00\u4e2a\u663e\u5f0f\u7684\u7ed3\u6784\u3002\u5404\u4e2a\u8fd0\u7b97\u7b26\u5212\u5206\u6210\u5355\u72ec\u7684\u7c7b\uff08\u540c\u65f6\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5\uff09\uff0c\u5e76\u7edf\u4e00\u7ee7\u627f\u4e00\u4e2a\u7c7b\u4f3cIterator\u7684\u7236\u7c7b\uff1a trait ExecutionOperator extends TreeNode { // Input operators val children: Seq[ExecutionOperator] // Prepare for processing. def Open() = { ...common logic... OpenImpl() } // To be implemented by concrete operators. def OpenSelf(); // Fills output_batch. Empty RowBatch means the end of output. def GetNext(output_batch: RowBatch) = { ...common logic... // Example way of collecting per-operator metrics. ScopedTimer timer; return GetNextImpl(output_batch); } // Interface for child class to override def GetNextImpl(output_batch: RowBatch); // Ends processing. def Close() = { ...common logic... CloseImpl() } // To be implemented by concrete operators. def CloseImpl(); // Schema of output rows. def OutputSchema() } // Example concrete operator. class FilterOperator extends ExecutionOperator { override def OpenImpl() = { children[0].Open() } override def GetNextImpl(output_batch: RowBatch): Unit = { while (!output_batch.full()) { RowBatch input_batch; children[0].GetNext(input_batch) if (input_batch.empty()) break; RunFilter(input_batch, output_batch) } } override def CloseImpl() = { children[0].Close() } } \u8fd0\u7b97\u7b26\u7684\u6574\u4e2a\u5468\u671f\u662fOpen->GetNext->Close\uff0c\u5176\u4e2dGetNext\u53ef\u4ee5\u53cd\u590d\u8c03\u7528\u76f4\u5230\u4e0d\u80fd\u518d\u8fd4\u56de\u66f4\u591a\u7ed3\u679c\u3002\u7236\u7c7bExecutionOperator\u4e2d\u5bf9\u8fd9\u4e09\u4e2a\u63a5\u53e3\u6709\u7edf\u4e00\u7684\u57fa\u672c\u5b9e\u73b0\uff0c\u7528\u6765\u505a\u4e00\u4e9b\u6240\u6709\u64cd\u4f5c\u7b26\u90fd\u901a\u7528\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982\u6536\u96c6metrics\uff0c\u5404\u64cd\u4f5c\u7b26\u72ec\u81ea\u7684\u903b\u8f91\u5728\u5bf9\u4e8e\u7684Impl\u65b9\u6cd5\u91cc\u5b9e\u73b0\u3002\u5728\u6267\u884c\u8ba1\u5212\u751f\u6210\u540e\uff0c\u53ea\u9700\u50cf\u5bf9Iterator\u4e00\u6837\u4ece\u6839\u8282\u70b9\u83b7\u53d6\u8f93\u51fa\uff0c\u5404\u8282\u70b9\u5185\u90e8\u4f1a\u5c42\u7ea7\u8c03\u7528\u5404\u81ea\u5b50\u7ed3\u70b9\u7684Open/GetNext/Close\u3002 ExecutionOperator execution_plan = GenerateExecutionPlan ( physical_plan ) execution_plan . Open () while ( true ) { RowBatch output_batch ; execution_plan . GetNext ( output_batch ) ; if ( output_batch . empty ()) break ; // consume output here } execution_plan . Close () \u8fd0\u7b97\u7b26\u4e4b\u95f4\u4f20\u9012\u7684\u5355\u5143\u4e3aRowBatch\uff0c\u5305\u542b\u591a\u4e2a\u8f93\u51fa\u884c\u3002\u5177\u4f53\u884c\u6570\u53ef\u4ee5\u5148\u7528\u56fa\u5b9a\u503c\u6bd4\u5982256\u3002\u5c06\u6765\u6709\u5185\u5b58\u7ba1\u7406\u540e\u9700\u8981\u6309\u7167\u5b9e\u9645\u5360\u7528\u5185\u5b58\u5927\u5c0f\u51b3\u5b9a\u884c\u6570\u3002 class RowBatch { val rows: Seq[Seq[LynxValue]] } \u4ece\u7269\u7406\u8ba1\u5212\u5230\u6267\u884c\u8ba1\u5212\u7684\u8f6c\u6362\u65b9\u5f0f\u53ef\u4ee5\u53c2\u7167\u903b\u8f91\u8ba1\u5212\u5230\u7269\u7406\u8ba1\u5212\u7684\u8f6c\u6362\u3002","title":"\u65b9\u6848"},{"location":"designs/cn/performance-based-refactoring/#_5","text":"\u6dfb\u52a0ExecutionOperator\u548cRowBatch\u57fa\u672c\u7c7b\u522b \u9488\u5bf9\u6bcf\u79cd\u8fd0\u7b97\uff0c\u6dfb\u52a0\u4e00\u4e2a\u7ee7\u627fExecutionOperator\u8fd0\u7b97\u7b26\uff0c\u903b\u8f91\u53ef\u4ee5\u7167\u6284\u76f8\u5e94\u7684\u7269\u7406\u8ba1\u5212\u8282\u70b9\u7684execute\u65b9\u6cd5\u53caDataFrameOperator\u4e2d\u7684\u8fd0\u7b97\u65b9\u6cd5\uff08\u5982select/join)\uff0c\u7136\u540e\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5\u3002\u5355\u5143\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u5355\u4e2a\u8fd0\u7b97\u7b26\u6267\u884c\u5b8c\u6574\u7684Open/GetNext/Close\u5468\u671f\uff0c\u7136\u540e\u68c0\u67e5\u7ed3\u679c\u662f\u5426\u6b63\u786e\u3002\u4e3a\u4e86\u65b9\u4fbf\u6d4b\u8bd5\uff0c\u589e\u52a0\u4e00\u4e2a\u53ef\u5b9a\u5236\u8f93\u51fa\u6570\u636eFakeOperator\uff0c\u7528\u505a\u88ab\u6d4b\u8fd0\u7b97\u7b26\u7684\u8f93\u5165\u8fd0\u7b97\u7b26\u3002 \u6240\u6709\u8fd0\u7b97\u7b26\u6dfb\u52a0\u5b8c\u6bd5\u540e\uff0c\u6dfb\u52a0ExecutionPlanGenerator\uff0c\u7528\u4e8e\u5c06\u7269\u7406\u8ba1\u5212\u8f6c\u4e3a\u6267\u884c\u8ba1\u5212\u3002 \u5c06ExecutionPlanGenerator\u5e94\u7528\u5728CypherRunner\u4e2d\uff0c\u53d6\u4ee3physicalPlan.execute()\u3002\u5904\u7406\u8f93\u51fa\u65f6\uff0c\u53ef\u4ee5\u5199\u4e00\u4e2aIterator\u5c06\u6700\u7ec8\u7684RowBatch stream\u8f6c\u4e3aLynxResult.records\u8981\u6c42\u7684stream\uff0c\u8fd9\u6837\u53ef\u907f\u514d\u4f7f\u7528LynxResult\u7684\u5176\u4ed6\u90e8\u4ef6\u3002 \u6e05\u7406DataFrame/DataFrameOperator/DataFrameOps\u53ca\u6240\u6709\u7684PPTNode\u5b50\u7c7b\u7684execute\u65b9\u6cd5","title":"\u5b9e\u65bd"},{"location":"designs/en/decypher/","text":"De-Cypher \u00b6 Problem \u00b6 Lynx relies on openCypher for parsing queries. However, Cypher constructs penetrates almost all layers of Lynx besides parsing, including logical plan, physical plan and execution plan. Take projection as example: LogicalProject has Cypher ReturnItemsDef; PhysicalProject has its too; ProjectOperator doesn\u2019t have it, but still has Cypher Expression. The same pattern appears in most operators. This poses several problems. * Cypher concepts don\u2019t always map to Lynx concepts. For example, Cypher ReturnItemsDef represents a list of items in RETURN clause, which can be simple projection or complex aggregation. After being converted to logical/physical plan, it can be vastly different operators. * (As a result of item 1) Cypher constructs have information that\u2019s unnecessary for different stages of query processing. For example, for physical Project, we only need a list of Expressions for projection, but Cypher ReturnItemsDef much more than that. As a result, when we need to build an Expression, we have to set many unnecessary parameters. For example, all Cypher expressions require a position parameter (because Cypher needs to report where parsing error happens), but this is totally unnecessary for query planning/execution. * (As a result of item 1) We cannot easily change Cypher for our convenience. For example, if we want to fixate intermediate row schema to certain offsets, there is no easy way to attach offsets Cypher Variables. Also, there is no type info in Cypher Expression and we cannot easily add it, which makes type-specific (for example, null or non-null) optimizations impossible. Incompatible Cypher changes can break relevant parts in Lynx. Solution \u00b6 We should only use openCypher for parsing. During logical planning, we should convert Cypher constructs to our own representation and keep using it for the rest of query processing. The gap between current state and the desired state includes three parts: * Logical nodes. Existing logical nodes are mostly wrappers of relevant Cypher constructs. We should extract useful info from Cypher constructs and add them as fields of logical node. For example, LogicalProject should only get the list of projection expression and output column names. * Physical nodes. Same as logical node. * Expressions. This requires our own Expression representation. It will be similar to Cypher Expression. But we can trim/add fields according to our need. There are two main changes we need for now: * Add offset in Variable expression. This will allow us to get variable value by offset instead of name lookup during evaluation. * Add type info in each expression. Leaf nodes of Expression tree like Literal or Variable should have given type. Intermediate node like FunctionCall should be able to deduct the output type. Implementation plan \u00b6 Since expression library is the cornerstone, we will start with adding our own version. Instead of adding the entire expression library and changing all layers in one shot, we can actually do it incrementally. First we can add base class LynxExpress and make it extend Cypher Expression. Then we add a conversion util for converting Cypher Expressions that are supported by our expression library to our own version. In each logical translation, when we see any Cypher Expression, we call this util to rewrite the expression tree. Notice this rewrite can happen at any level of expression tree, for example, in Add(VariableA, VariableB), we might support Variable before Add. And the tree after rewrite can have both Cypher Expression and Lynx Expression. Then we can add concrete expression one by one without breaking existing code. Each concrete Expression is a subclass of base LynxExpression. After adding each one, we should also update the expression evaluator to correctly process it and update the conversion util to do the rewrite. Given there is dependency from execution operator to physical node and from physical node to logical node, we should implement the change from execution layer first and then physical planning and logical planning. Eventually, after all changes are done, we can stop making LynxExpression extend Cypher Expression and this will be the final cut between Lynx expression and Cypher expression.","title":"Decypher"},{"location":"designs/en/decypher/#de-cypher","text":"","title":"De-Cypher"},{"location":"designs/en/decypher/#problem","text":"Lynx relies on openCypher for parsing queries. However, Cypher constructs penetrates almost all layers of Lynx besides parsing, including logical plan, physical plan and execution plan. Take projection as example: LogicalProject has Cypher ReturnItemsDef; PhysicalProject has its too; ProjectOperator doesn\u2019t have it, but still has Cypher Expression. The same pattern appears in most operators. This poses several problems. * Cypher concepts don\u2019t always map to Lynx concepts. For example, Cypher ReturnItemsDef represents a list of items in RETURN clause, which can be simple projection or complex aggregation. After being converted to logical/physical plan, it can be vastly different operators. * (As a result of item 1) Cypher constructs have information that\u2019s unnecessary for different stages of query processing. For example, for physical Project, we only need a list of Expressions for projection, but Cypher ReturnItemsDef much more than that. As a result, when we need to build an Expression, we have to set many unnecessary parameters. For example, all Cypher expressions require a position parameter (because Cypher needs to report where parsing error happens), but this is totally unnecessary for query planning/execution. * (As a result of item 1) We cannot easily change Cypher for our convenience. For example, if we want to fixate intermediate row schema to certain offsets, there is no easy way to attach offsets Cypher Variables. Also, there is no type info in Cypher Expression and we cannot easily add it, which makes type-specific (for example, null or non-null) optimizations impossible. Incompatible Cypher changes can break relevant parts in Lynx.","title":"Problem"},{"location":"designs/en/decypher/#solution","text":"We should only use openCypher for parsing. During logical planning, we should convert Cypher constructs to our own representation and keep using it for the rest of query processing. The gap between current state and the desired state includes three parts: * Logical nodes. Existing logical nodes are mostly wrappers of relevant Cypher constructs. We should extract useful info from Cypher constructs and add them as fields of logical node. For example, LogicalProject should only get the list of projection expression and output column names. * Physical nodes. Same as logical node. * Expressions. This requires our own Expression representation. It will be similar to Cypher Expression. But we can trim/add fields according to our need. There are two main changes we need for now: * Add offset in Variable expression. This will allow us to get variable value by offset instead of name lookup during evaluation. * Add type info in each expression. Leaf nodes of Expression tree like Literal or Variable should have given type. Intermediate node like FunctionCall should be able to deduct the output type.","title":"Solution"},{"location":"designs/en/decypher/#implementation-plan","text":"Since expression library is the cornerstone, we will start with adding our own version. Instead of adding the entire expression library and changing all layers in one shot, we can actually do it incrementally. First we can add base class LynxExpress and make it extend Cypher Expression. Then we add a conversion util for converting Cypher Expressions that are supported by our expression library to our own version. In each logical translation, when we see any Cypher Expression, we call this util to rewrite the expression tree. Notice this rewrite can happen at any level of expression tree, for example, in Add(VariableA, VariableB), we might support Variable before Add. And the tree after rewrite can have both Cypher Expression and Lynx Expression. Then we can add concrete expression one by one without breaking existing code. Each concrete Expression is a subclass of base LynxExpression. After adding each one, we should also update the expression evaluator to correctly process it and update the conversion util to do the rewrite. Given there is dependency from execution operator to physical node and from physical node to logical node, we should implement the change from execution layer first and then physical planning and logical planning. Eventually, after all changes are done, we can stop making LynxExpression extend Cypher Expression and this will be the final cut between Lynx expression and Cypher expression.","title":"Implementation plan"},{"location":"designs/en/protocol-services-interface-design/","text":"Protocol, Services, and Interfaces Design \u00b6 Protocol Design \u00b6 Object Definitions \u00b6 Below are the main objects in the protocols. Our services are built on these objects. // Element is the basic structure for both Node and Relationship . It 's ID must be unique to its associated Node or Relationship. // An element also maintains a collection of Properties . An element must belong to a graph . message Element { int32 id = 1 ; string name = 2 ; // The graph ID associated with this element . int32 graph_id = 3 ; repeated Property properties = 4 ; } // Property holds a key - value pair that 's associated with an element. message Property { // The element ID associated with this property . int32 element_id = 1 ; string key = 2 ; google . protobuf . Any value = 3 ; } // Graph is a collection of nodes and relationships . message Graph { int32 id = 1 ; string name = 2 ; repeated Node nodes = 3 ; repeated Relationship relationships = 4 ; } // Node extends Element and connects to relationships . message Node { // The following fields are inherited from Element but we write them here explictly since proto3 does not support inheritance . int32 id = 1 string name = 2 ; int32 graph_id = 3 ; // The graph ID associated with this node . repeated Property properties = 4 ; repeated string labels = 5 ; Relationship in_relation = 6 ; Relationship out_relation = 7 ; } // Relationship extends Element and connects two nodes . message Relationship { // The following fields are inherited from Element but we write them here explictly since proto3 does not support inheritance . string id = 1 string name = 2 ; int32 graph_id = 3 ; // The graph ID associated with this relationship . repeated Property properties = 4 ; Node start_node = 5 ; Node end_node = 6 ; // The type of relationship between the two connected nodes . string relationType = 7 ; } Generic Query \u00b6 We support generic query for advanced users to send query directly to the database: The request only contains statement: message GenericQueryRequest { string statement = 1; } The response contains results (e.g. graph objects), message, and exit code: message GenericResponseStatus { string message = 2; int32 exit_code = 3; } message GenericQueryResponse { repeated map<string, google.protobuf.Any> results = 1; GenericResponseStatus status = 2; } The service definition is as follows: service QueryService { rpc Query(GenericQueryRequest) returns (stream GenericQueryResponse) {} } Object-specific Services \u00b6 We support object-specific services such as create/get/delete graphs to provide easy-to-use endpoints to our REST client and high-level SDKs. Let's use graph service as an example: message GraphCreateRequest { Graph graph = 1 ; } ; message GraphCreateResponse { Graph graph = 1 ; GenericResponseStatus status = 2 ; } ; message GraphGetRequest { string name = 1 ; } ; message GraphGetResponse { Graph graph = 1 ; GenericResponseStatus status = 2 ; } ; message GraphListRequest { // TODO : Some filtering options } ; message GraphListResponse { repeated Graph graphs = 1 ; GenericResponseStatus status = 2 ; } ; message GraphDeleteRequest { string name = 1 ; } ; message GraphDeleteResponse { GenericResponseStatus status = 1 ; } ; service GraphService { rpc CreateGraph(GraphCreateRequest) returns GraphCreateResponse { option (google.api.http) = { post : \"/api/v1/graphs/{graph_name}\" body : \"*\" } ; } rpc GetGraph ( GraphGetRequest ) returns GraphGetResponse { option (google.api.http).get = \"/api/v1/graphs/{name } \"; } rpc ListGraphs(GraphListRequest) returns GraphListResponse { option (google.api.http).get = \" / api / v1 / graphs \"; } rpc DeleteGraph(GraphDeleteRequest) returns GraphDeleteResponse { option (google.api.http).get = \" / api / v1 / graphs / { name } \" ; } } Services for other objects are similar. Interfaces \u00b6 REST APIs \u00b6 We provide three types of methods: GET: Obtain data from the database; POST: Send/modify data to the database; DELETE: Delete data from the database. These three methods are available for the all the core objects in the graph database. Note that all following endpoints will be prefixed by /api/v1 so that this document is more concise. For example, the following endpoints are available for graphs: GET /graphs : obtain a list of graphs; GET /graphs/<graph_name> : obtain a particular graph; POST /graphs/<graph_name> : create a new graph in the database; DELETE /graphs/<graph_name> : delete an existing graph. The following endpoints are available for nodes: 1. GET /graphs/<graph_name>/nodes : obtain a list of nodes in a particular graph; 2. GET /graphs/<graph_name>/nodes/labels : obtain a list of nodes in a particular graph by labels; 3. GET /graphs/<graph_name>/nodes/<node_name> : get a particular node from a graph; 4. POST /graphs/<graph_name>/nodes/<node_name> : create a new node in the graph; 5. DELETE /graphs/<graph_name>/nodes/<node_name> : delete an existing node; 6. DELETE /graphs/<graph_name>/nodes : delete all nodes in the graph; 7. DELETE /graphs/<graph_name>/nodes/labels : delete all nodes in the graph by labels. The endpoints are available for relationships are similar. Each endpoint also allows specifying query parameters, e.g. GET /graphs/<graph_name>/?<query_params> . The returned object looks like the following: { \"version\": { \"api\": \"v2\", \"schema\": 0 }, \"exitCode\": 0, \"message\": \"successfully retrieved\", \"results\": [ { \"id\": \"0\", \"nodes\": [...] }, ... ] } OpenAPI Spec \u00b6 REST APIs can be generated via grpc-gateway . We can automatically generate OpenAPI spec as well. SDKs \u00b6 We also provide SDKs such as Python SDK to our users. First, initialize the client: client = tudb . Client ( address = xxx , config = xxx ) We can then get the graphs and nodes similar to REST APIs: // Get the list of graphs client . list_graphs () // Get a particular graph graph = client . get_graph ( name = xxx ) // Create a new graph graph = client . create_graph ( Graph ( ... )) // Delete a graph client . delete_graph ( name = xxx ) // Get the list of the nodes associated with the graph graph . list_nodes ( labels = xxx ) // Get a particular node graph . get_node ( name = xxx ) // Create a node graph . create_node ( Node ( ... )) // Delete a node graph . delete_node ( name = xxx ) // Get the list of relationships associated with the graph graph . list_relationships () // Get a particular relationship graph . get_relationship ( name = xxx ) // Create a relationship graph . create_relationship ( Relationship ( ... )) // Delete a relationship graph . delete_relationship ( name = xxx ) CLI \u00b6 Our CLI provides the following commands where object_type can be one of the following: graph, node, and relationship. tudb list <object_type> : get a list of objects of this type. tudb get <object_type> <object_name> : get a single object of this type. tudb create <object_type> -f <path_to_graph_definition> : create an object of this type based on the specified definition. tudb delete <object_type> <object_name> : delete a single object of this type. tudb delete <object_type> --all : delete all objects of this type. For nodes, there are also additional flags: 1. --labels : retrieving nodes that have this list of labels. For both nodes and relationships, we also need to provide --graph <graph_name> to associate with a graph when running the queries. Implementation Plan \u00b6 Some decisions after discussions: The current protobuf definition for core objects are unnecessarily complex and designed to support Gremlin in the future. However, we'd like to simplify the implementation of the service so that those definitions (e.g. Node and Relationship) are as close to the current storage API as possible. For example, Element and Property are unnecessary and can be removed for the initial implementation. We can revisit them later once we re-evaluate whether/how to support Gremlin. GraphService , NodeService , and RelationshipService focus on providing simple get/create/list methods that can be used for REST/CLI/Python clients. They will be implemented based on storage APIs. However, if users want to perform complex queries, the existing TuDBServer will be leveraged instead. The implementation plan for the Scala gRPC services consists of the following tasks: Add get/create/list methods in GraphService , NodeService , and RelationshipService that implement GraphServiceGrpc.GraphServiceImplBase (without integration with storage yet). Implement NodeService that uses NodeStoreAPI . For each node, we will include a graph_id in StoredNodeWithProperty 's properties so that later we can use it to look up nodes that belong to a graph in GraphService . Implement RelationshipService that uses RelationshipStoreAPI . Similarly, we will include a reference to the graph it belongs to in StoredRelationshipWithProperty 's properties. Implement GraphService that constructs a graph object with the list of nodes and relationships. Since the graph can get extremely large, we will support parameters like nodeCount and relationshipCount similar to Neo4j's graph.list() method to allow getting a subset of the nodes and relationships. The next phase of the development will be building the RESTful, CLI and Python interfaces based on the gRPC services implemented above. 1. RESTful APIs can be generated via grpc-gateway . We can automatically generate OpenAPI spec as well. 2. CLI will be built in Scala that invokes the gRPC services. 3. Python interface will be implemented in Python via gRPC calls to the running Scala gRPC services. References \u00b6 Gremlin structure APIs Neo4j Gremlin structure APIs TigerGraph REST APIs","title":"Protocol, Services, and Interfaces Design"},{"location":"designs/en/protocol-services-interface-design/#protocol-services-and-interfaces-design","text":"","title":"Protocol, Services, and Interfaces Design"},{"location":"designs/en/protocol-services-interface-design/#protocol-design","text":"","title":"Protocol Design"},{"location":"designs/en/protocol-services-interface-design/#object-definitions","text":"Below are the main objects in the protocols. Our services are built on these objects. // Element is the basic structure for both Node and Relationship . It 's ID must be unique to its associated Node or Relationship. // An element also maintains a collection of Properties . An element must belong to a graph . message Element { int32 id = 1 ; string name = 2 ; // The graph ID associated with this element . int32 graph_id = 3 ; repeated Property properties = 4 ; } // Property holds a key - value pair that 's associated with an element. message Property { // The element ID associated with this property . int32 element_id = 1 ; string key = 2 ; google . protobuf . Any value = 3 ; } // Graph is a collection of nodes and relationships . message Graph { int32 id = 1 ; string name = 2 ; repeated Node nodes = 3 ; repeated Relationship relationships = 4 ; } // Node extends Element and connects to relationships . message Node { // The following fields are inherited from Element but we write them here explictly since proto3 does not support inheritance . int32 id = 1 string name = 2 ; int32 graph_id = 3 ; // The graph ID associated with this node . repeated Property properties = 4 ; repeated string labels = 5 ; Relationship in_relation = 6 ; Relationship out_relation = 7 ; } // Relationship extends Element and connects two nodes . message Relationship { // The following fields are inherited from Element but we write them here explictly since proto3 does not support inheritance . string id = 1 string name = 2 ; int32 graph_id = 3 ; // The graph ID associated with this relationship . repeated Property properties = 4 ; Node start_node = 5 ; Node end_node = 6 ; // The type of relationship between the two connected nodes . string relationType = 7 ; }","title":"Object Definitions"},{"location":"designs/en/protocol-services-interface-design/#generic-query","text":"We support generic query for advanced users to send query directly to the database: The request only contains statement: message GenericQueryRequest { string statement = 1; } The response contains results (e.g. graph objects), message, and exit code: message GenericResponseStatus { string message = 2; int32 exit_code = 3; } message GenericQueryResponse { repeated map<string, google.protobuf.Any> results = 1; GenericResponseStatus status = 2; } The service definition is as follows: service QueryService { rpc Query(GenericQueryRequest) returns (stream GenericQueryResponse) {} }","title":"Generic Query"},{"location":"designs/en/protocol-services-interface-design/#object-specific-services","text":"We support object-specific services such as create/get/delete graphs to provide easy-to-use endpoints to our REST client and high-level SDKs. Let's use graph service as an example: message GraphCreateRequest { Graph graph = 1 ; } ; message GraphCreateResponse { Graph graph = 1 ; GenericResponseStatus status = 2 ; } ; message GraphGetRequest { string name = 1 ; } ; message GraphGetResponse { Graph graph = 1 ; GenericResponseStatus status = 2 ; } ; message GraphListRequest { // TODO : Some filtering options } ; message GraphListResponse { repeated Graph graphs = 1 ; GenericResponseStatus status = 2 ; } ; message GraphDeleteRequest { string name = 1 ; } ; message GraphDeleteResponse { GenericResponseStatus status = 1 ; } ; service GraphService { rpc CreateGraph(GraphCreateRequest) returns GraphCreateResponse { option (google.api.http) = { post : \"/api/v1/graphs/{graph_name}\" body : \"*\" } ; } rpc GetGraph ( GraphGetRequest ) returns GraphGetResponse { option (google.api.http).get = \"/api/v1/graphs/{name } \"; } rpc ListGraphs(GraphListRequest) returns GraphListResponse { option (google.api.http).get = \" / api / v1 / graphs \"; } rpc DeleteGraph(GraphDeleteRequest) returns GraphDeleteResponse { option (google.api.http).get = \" / api / v1 / graphs / { name } \" ; } } Services for other objects are similar.","title":"Object-specific Services"},{"location":"designs/en/protocol-services-interface-design/#interfaces","text":"","title":"Interfaces"},{"location":"designs/en/protocol-services-interface-design/#rest-apis","text":"We provide three types of methods: GET: Obtain data from the database; POST: Send/modify data to the database; DELETE: Delete data from the database. These three methods are available for the all the core objects in the graph database. Note that all following endpoints will be prefixed by /api/v1 so that this document is more concise. For example, the following endpoints are available for graphs: GET /graphs : obtain a list of graphs; GET /graphs/<graph_name> : obtain a particular graph; POST /graphs/<graph_name> : create a new graph in the database; DELETE /graphs/<graph_name> : delete an existing graph. The following endpoints are available for nodes: 1. GET /graphs/<graph_name>/nodes : obtain a list of nodes in a particular graph; 2. GET /graphs/<graph_name>/nodes/labels : obtain a list of nodes in a particular graph by labels; 3. GET /graphs/<graph_name>/nodes/<node_name> : get a particular node from a graph; 4. POST /graphs/<graph_name>/nodes/<node_name> : create a new node in the graph; 5. DELETE /graphs/<graph_name>/nodes/<node_name> : delete an existing node; 6. DELETE /graphs/<graph_name>/nodes : delete all nodes in the graph; 7. DELETE /graphs/<graph_name>/nodes/labels : delete all nodes in the graph by labels. The endpoints are available for relationships are similar. Each endpoint also allows specifying query parameters, e.g. GET /graphs/<graph_name>/?<query_params> . The returned object looks like the following: { \"version\": { \"api\": \"v2\", \"schema\": 0 }, \"exitCode\": 0, \"message\": \"successfully retrieved\", \"results\": [ { \"id\": \"0\", \"nodes\": [...] }, ... ] }","title":"REST APIs"},{"location":"designs/en/protocol-services-interface-design/#openapi-spec","text":"REST APIs can be generated via grpc-gateway . We can automatically generate OpenAPI spec as well.","title":"OpenAPI Spec"},{"location":"designs/en/protocol-services-interface-design/#sdks","text":"We also provide SDKs such as Python SDK to our users. First, initialize the client: client = tudb . Client ( address = xxx , config = xxx ) We can then get the graphs and nodes similar to REST APIs: // Get the list of graphs client . list_graphs () // Get a particular graph graph = client . get_graph ( name = xxx ) // Create a new graph graph = client . create_graph ( Graph ( ... )) // Delete a graph client . delete_graph ( name = xxx ) // Get the list of the nodes associated with the graph graph . list_nodes ( labels = xxx ) // Get a particular node graph . get_node ( name = xxx ) // Create a node graph . create_node ( Node ( ... )) // Delete a node graph . delete_node ( name = xxx ) // Get the list of relationships associated with the graph graph . list_relationships () // Get a particular relationship graph . get_relationship ( name = xxx ) // Create a relationship graph . create_relationship ( Relationship ( ... )) // Delete a relationship graph . delete_relationship ( name = xxx )","title":"SDKs"},{"location":"designs/en/protocol-services-interface-design/#cli","text":"Our CLI provides the following commands where object_type can be one of the following: graph, node, and relationship. tudb list <object_type> : get a list of objects of this type. tudb get <object_type> <object_name> : get a single object of this type. tudb create <object_type> -f <path_to_graph_definition> : create an object of this type based on the specified definition. tudb delete <object_type> <object_name> : delete a single object of this type. tudb delete <object_type> --all : delete all objects of this type. For nodes, there are also additional flags: 1. --labels : retrieving nodes that have this list of labels. For both nodes and relationships, we also need to provide --graph <graph_name> to associate with a graph when running the queries.","title":"CLI"},{"location":"designs/en/protocol-services-interface-design/#implementation-plan","text":"Some decisions after discussions: The current protobuf definition for core objects are unnecessarily complex and designed to support Gremlin in the future. However, we'd like to simplify the implementation of the service so that those definitions (e.g. Node and Relationship) are as close to the current storage API as possible. For example, Element and Property are unnecessary and can be removed for the initial implementation. We can revisit them later once we re-evaluate whether/how to support Gremlin. GraphService , NodeService , and RelationshipService focus on providing simple get/create/list methods that can be used for REST/CLI/Python clients. They will be implemented based on storage APIs. However, if users want to perform complex queries, the existing TuDBServer will be leveraged instead. The implementation plan for the Scala gRPC services consists of the following tasks: Add get/create/list methods in GraphService , NodeService , and RelationshipService that implement GraphServiceGrpc.GraphServiceImplBase (without integration with storage yet). Implement NodeService that uses NodeStoreAPI . For each node, we will include a graph_id in StoredNodeWithProperty 's properties so that later we can use it to look up nodes that belong to a graph in GraphService . Implement RelationshipService that uses RelationshipStoreAPI . Similarly, we will include a reference to the graph it belongs to in StoredRelationshipWithProperty 's properties. Implement GraphService that constructs a graph object with the list of nodes and relationships. Since the graph can get extremely large, we will support parameters like nodeCount and relationshipCount similar to Neo4j's graph.list() method to allow getting a subset of the nodes and relationships. The next phase of the development will be building the RESTful, CLI and Python interfaces based on the gRPC services implemented above. 1. RESTful APIs can be generated via grpc-gateway . We can automatically generate OpenAPI spec as well. 2. CLI will be built in Scala that invokes the gRPC services. 3. Python interface will be implemented in Python via gRPC calls to the running Scala gRPC services.","title":"Implementation Plan"},{"location":"designs/en/protocol-services-interface-design/#references","text":"Gremlin structure APIs Neo4j Gremlin structure APIs TigerGraph REST APIs","title":"References"},{"location":"examples/basic/","text":"Basic Examples \u00b6 TBA","title":"Basic Examples"},{"location":"examples/basic/#basic-examples","text":"TBA","title":"Basic Examples"}]}